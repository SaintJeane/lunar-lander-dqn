# Architecture Documentation

This document provides detailed technical documentation of the DQN LunarLander implementation.

## ğŸ“ System Architecture

### High-Level Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DQN Training System                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚              â”‚      â”‚              â”‚                     â”‚
â”‚  â”‚  Environment â”‚â—„â”€â”€â”€â”€â–ºâ”‚  DQN Agent   â”‚                     â”‚
â”‚  â”‚  (Gymnasium) â”‚      â”‚              â”‚                     â”‚
â”‚  â”‚              â”‚      â”‚              â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                               â”‚                             â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚                    â”‚          â”‚          â”‚                  â”‚
â”‚             â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚             â”‚  Policy   â”‚ â”‚Target â”‚ â”‚  Replay  â”‚            â”‚
â”‚             â”‚  Network  â”‚ â”‚Networkâ”‚ â”‚  Buffer  â”‚            â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§© Component Details

### 1. DQN Neural Network (`DQN` class)

**Purpose**: Approximate the Q-value function Q(s, a)

**Architecture**:
```
Input (8) â†’ Linear(128) â†’ ReLU â†’ Linear(128) â†’ ReLU â†’ Output(4)
```

**Key Methods**:
- `forward(x)`: Compute Q-values for input state

**Design Decisions**:
- **Two hidden layers**: Balance between capacity and training speed
- **128 neurons**: Sufficient for 8D state space, not excessive
- **ReLU activation**: Standard, prevents vanishing gradients
- **No dropout**: DQN typically doesn't need regularization

**Mathematical Formulation**:
```
hâ‚ = ReLU(Wâ‚x + bâ‚)
hâ‚‚ = ReLU(Wâ‚‚hâ‚ + bâ‚‚)
Q(s,a) = Wâ‚ƒhâ‚‚ + bâ‚ƒ
```

### 2. Experience Replay Buffer (`ReplayBuffer` class)

**Purpose**: Store and sample past experiences to break correlation

**Data Structure**: `collections.deque` with maximum capacity

**Storage Format**:
```python
(state, action, reward, next_state, done)
```

**Key Methods**:
- `push(s, a, r, s', done)`: Add experience
- `sample(batch_size)`: Random sampling
- `__len__()`: Buffer size

**Implementation Details**:
```python
class ReplayBuffer:
    def __init__(self, capacity=100000):
        self.buffer = deque(maxlen=capacity)  # Circular buffer
    
    def push(self, *transition):
        self.buffer.append(transition)
    
    def sample(self, batch_size):
        # Random uniform sampling
        batch = random.sample(self.buffer, batch_size)
        return zip(*batch)
```

**Why Replay Buffer?**:
1. **Breaks correlation**: Sequential experiences are highly correlated
2. **Sample efficiency**: Reuse past experiences multiple times
3. **Stabilizes training**: Reduces variance in gradient updates

### 3. DQN Agent (`DQNAgent` class)

**Purpose**: Orchestrate training, maintain networks, handle exploration

**Components**:
- Policy Network (Q_policy): Current best estimate
- Target Network (Q_target): Stabilized target for TD updates
- Replay Buffer: Experience storage
- Optimizer: Adam with learning rate 1e-3

**Key Methods**:

#### `select_action(state, training=True)`
Epsilon-greedy action selection:
```python
if training and random() < epsilon:
    return random_action()  # Explore
else:
    return argmax(Q_policy(state))  # Exploit
```

#### `update()`
Core training logic:
```python
# Sample batch
batch = replay_buffer.sample(batch_size)

# Current Q-values
Q_current = Q_policy(states)[actions]

# Target Q-values (using target network)
Q_next = Q_target(next_states).max(dim=1)
Q_target_values = rewards + gamma * Q_next * (1 - dones)

# Compute loss and update
loss = SmoothL1Loss(Q_current, Q_target_values)
optimizer.zero_grad()
loss.backward()
clip_grad_norm_(parameters, max_norm=1.0)
optimizer.step()

# Update target network periodically
if steps % target_update_freq == 0:
    Q_target.load_state_dict(Q_policy.state_dict())
```

#### `decay_epsilon()`
Exponential decay:
```python
epsilon = max(epsilon_end, epsilon * epsilon_decay)
```

## ğŸ”„ Training Loop

### Complete Training Flow

```python
for episode in range(num_episodes):
    state = env.reset()
    episode_reward = 0
    
    for step in range(max_steps):
        # 1. Select action
        action = agent.select_action(state)
        
        # 2. Execute in environment
        next_state, reward, done, _ = env.step(action)
        
        # 3. Store transition
        agent.replay_buffer.push(state, action, reward, next_state, done)
        
        # 4. Update agent
        loss = agent.update()
        
        # 5. Move to next state
        episode_reward += reward
        state = next_state
        
        if done:
            break
    
    # 6. Decay exploration
    agent.decay_epsilon()
    
    # 7. Log and save
    if episode % save_interval == 0:
        agent.save(f"checkpoint_{episode}.pth")
```

### Update Mechanism

The DQN update follows the Bellman equation:

**Target**:
```
y = r + Î³ max Q_target(s', a')
              a'
```

**Loss**:
```
L = SmoothL1(Q_policy(s, a) - y)
```

**Why Smooth L1 Loss?**:
- Less sensitive to outliers than MSE
- More stable gradients than absolute error
- Defined as:
  ```
  L(x) = { 0.5 * xÂ²     if |x| < 1
         { |x| - 0.5    otherwise
  ```

## ğŸ¯ Hyperparameter Tuning

### Critical Hyperparameters

| Parameter | Default | Impact | Tuning Advice |
|-----------|---------|--------|---------------|
| Learning Rate | 0.001 | Convergence speed | Too high: unstable; Too low: slow |
| Gamma (Î³) | 0.99 | Future reward weight | Higher: more long-term; Lower: more immediate |
| Epsilon Decay | 0.995 | Exploration schedule | Faster decay: exploit sooner; Slower: explore longer |
| Batch Size | 64 | Update stability | Larger: more stable, slower; Smaller: less stable, faster |
| Target Update Freq | 10 | Training stability | More frequent: faster but less stable |
| Buffer Size | 100K | Memory diversity | Larger: more diverse but more memory |

### Tuning Strategy

1. **Start with defaults** to establish baseline
2. **Learning rate search** (0.0001 to 0.01)
3. **Exploration tuning** based on convergence speed
4. **Buffer and batch size** based on memory constraints
5. **Fine-tune** target update frequency

## ğŸ”§ Implementation Details

### Gradient Clipping

```python
torch.nn.utils.clip_grad_norm_(parameters, max_norm=1.0)
```

**Purpose**: Prevent exploding gradients
**Effect**: Limits gradient norm to maximum value

### Target Network Update

**Soft Update** (not used here, but alternative):
```python
for target_param, policy_param in zip(target_net.parameters(), 
                                       policy_net.parameters()):
    target_param.data.copy_(tau * policy_param + (1-tau) * target_param)
```

**Hard Update** (current implementation):
```python
if steps % target_update_freq == 0:
    target_net.load_state_dict(policy_net.state_dict())
```

### Device Management

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```

Automatically uses GPU if available for faster training.

## ğŸ“Š Performance Optimization

### Memory Efficiency

1. **Replay Buffer**: Uses deque (O(1) append/pop)
2. **Detach tensors**: Use `torch.no_grad()` during evaluation
3. **Clear cache**: Delete unused tensors explicitly

```python
# Good practice
with torch.no_grad():
    q_values = policy_net(state)
```

### Computational Efficiency

**Vectorized Operations**:
```python
# Efficient: vectorized
q_values = policy_net(states)  # Batch of states

# Inefficient: loop
for state in states:
    q_value = policy_net(state)  # One at a time
```

**GPU Utilization**:
```python
# Transfer to GPU once
states = torch.FloatTensor(states).to(device)

# Not: repeated transfers
for state in states:
    state = torch.FloatTensor(state).to(device)  # Slow!
```

### Training Speed Benchmarks

| Configuration | Time/Episode | Episodes/Hour |
|--------------|--------------|---------------|
| CPU (i7-10700) | 5-8 seconds | 450-720 |
| GPU (RTX 3080) | 0.5-1 second | 3600-7200 |

## ğŸ› Common Issues and Solutions

### 1. Training Instability

**Symptoms**: Reward oscillates wildly, doesn't converge

**Causes**:
- Learning rate too high
- Target network updated too frequently
- Insufficient exploration

**Solutions**:
```python
# Reduce learning rate
learning_rate = 5e-4  # Instead of 1e-3

# Increase target update frequency
target_update_freq = 20  # Instead of 10

# Slower epsilon decay
epsilon_decay = 0.998  # Instead of 0.995
```

### 2. Slow Learning

**Symptoms**: Agent doesn't improve after many episodes

**Causes**:
- Learning rate too low
- Epsilon decays too slowly
- Network capacity insufficient

**Solutions**:
```python
# Increase learning rate
learning_rate = 2e-3

# Faster epsilon decay
epsilon_decay = 0.99

# Larger network
hidden_dim = 256  # Instead of 128
```

### 3. Catastrophic Forgetting

**Symptoms**: Performance drops suddenly after being good

**Causes**:
- Overwriting good experiences in buffer
- Target network updated too frequently

**Solutions**:
```python
# Larger replay buffer
buffer_capacity = 200000

# Less frequent target updates
target_update_freq = 50

# Prioritized experience replay (advanced)
```

### 4. Overestimation Bias

**Symptoms**: Agent is overly optimistic, takes risky actions

**Cause**: DQN tends to overestimate Q-values

**Solution**: Implement Double DQN
```python
# Standard DQN (overestimates)
max_q = Q_target(next_states).max(1)[0]

# Double DQN (less bias)
best_actions = Q_policy(next_states).argmax(1)
max_q = Q_target(next_states).gather(1, best_actions.unsqueeze(1)).squeeze(1)
```

## ğŸ”¬ Advanced Topics

### Double DQN Implementation

```python
def update_double_dqn(self):
    # ... sample batch ...
    
    # Use policy network to select actions
    next_actions = self.policy_net(next_states).argmax(1, keepdim=True)
    
    # Use target network to evaluate those actions
    next_q_values = self.target_net(next_states).gather(1, next_actions).squeeze(1)
    
    # Compute targets
    target_q_values = rewards + (1 - dones) * self.gamma * next_q_values
    
    # ... rest of update ...
```

### Dueling DQN Architecture

```python
class DuelingDQN(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim):
        super().__init__()
        
        # Shared feature extraction
        self.feature = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU()
        )
        
        # Value stream
        self.value = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        # Advantage stream
        self.advantage = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
    
    def forward(self, x):
        features = self.feature(x)
        value = self.value(features)
        advantage = self.advantage(features)
        
        # Combine value and advantage
        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))
        return q_values
```

### Prioritized Experience Replay

**Concept**: Sample important transitions more frequently

**Implementation**:
```python
class PrioritizedReplayBuffer:
    def __init__(self, capacity, alpha=0.6):
        self.capacity = capacity
        self.alpha = alpha  # Priority exponent
        self.buffer = []
        self.priorities = np.zeros(capacity, dtype=np.float32)
        self.position = 0
    
    def push(self, transition):
        max_priority = self.priorities.max() if self.buffer else 1.0
        
        if len(self.buffer) < self.capacity:
            self.buffer.append(transition)
        else:
            self.buffer[self.position] = transition
        
        self.priorities[self.position] = max_priority
        self.position = (self.position + 1) % self.capacity
    
    def sample(self, batch_size, beta=0.4):
        # Compute sampling probabilities
        priorities = self.priorities[:len(self.buffer)]
        probs = priorities ** self.alpha
        probs /= probs.sum()
        
        # Sample indices
        indices = np.random.choice(len(self.buffer), batch_size, p=probs)
        
        # Compute importance sampling weights
        total = len(self.buffer)
        weights = (total * probs[indices]) ** (-beta)
        weights /= weights.max()
        
        samples = [self.buffer[idx] for idx in indices]
        return samples, indices, weights
    
    def update_priorities(self, indices, priorities):
        for idx, priority in zip(indices, priorities):
            self.priorities[idx] = priority
```

## ğŸ“ˆ Evaluation Metrics

### Training Metrics

1. **Episode Reward**: Total reward per episode
2. **Moving Average**: Smoothed reward (100 episodes)
3. **Success Rate**: % of episodes with reward â‰¥ 200
4. **Training Loss**: TD-error magnitude
5. **Epsilon Value**: Current exploration rate

### Evaluation Metrics

1. **Mean Reward**: Average over evaluation episodes
2. **Standard Deviation**: Reward variance
3. **Success Rate**: Landing success percentage
4. **Episode Length**: Average steps to completion
5. **Action Distribution**: Frequency of each action

### Logging Example

```python
# During training
logger.log({
    'episode': episode,
    'reward': episode_reward,
    'moving_avg': moving_avg,
    'epsilon': agent.epsilon,
    'loss': avg_loss,
    'buffer_size': len(agent.replay_buffer)
})

# During evaluation
eval_metrics = {
    'mean_reward': np.mean(rewards),
    'std_reward': np.std(rewards),
    'success_rate': np.mean(np.array(rewards) >= 200),
    'mean_episode_length': np.mean(lengths)
}
```

## ğŸ“ Learning Resources

### Key Papers

1. **DQN** (Mnih et al., 2015)
   - "Human-level control through deep reinforcement learning"
   - Nature 518, 529-533

2. **Double DQN** (van Hasselt et al., 2016)
   - "Deep Reinforcement Learning with Double Q-learning"
   - AAAI 2016

3. **Dueling DQN** (Wang et al., 2016)
   - "Dueling Network Architectures for Deep Reinforcement Learning"
   - ICML 2016

4. **Prioritized Experience Replay** (Schaul et al., 2016)
   - "Prioritized Experience Replay"
   - ICLR 2016

5. **Rainbow** (Hessel et al., 2017)
   - "Rainbow: Combining Improvements in Deep Reinforcement Learning"
   - AAAI 2018

### Online Resources

- **OpenAI Spinning Up**: Comprehensive RL tutorial
- **DeepMind Lectures**: YouTube series on RL
- **Sutton & Barto**: "Reinforcement Learning: An Introduction"
- **PyTorch Tutorials**: Official DQN tutorial

## ğŸ”® Future Extensions

### Algorithmic Improvements

1. **Rainbow DQN**: Combine all DQN variants
2. **Noisy Networks**: Learnable exploration
3. **Distributional RL**: Model return distributions
4. **Multi-step Learning**: n-step returns

### Engineering Improvements

1. **Distributed Training**: Multiple workers
2. **Hyperparameter Optimization**: Optuna/Ray Tune
3. **Model Compression**: Pruning, quantization
4. **Production Deployment**: ONNX export, serving

### Environment Extensions

1. **Curriculum Learning**: Progressive difficulty
2. **Transfer Learning**: Pre-trained features
3. **Multi-task Learning**: Multiple environments
4. **Meta-learning**: Quick adaptation

## ğŸ“ Code Style Guide

### Naming Conventions

```python
# Classes: PascalCase
class DQNAgent:
    pass

# Functions/methods: snake_case
def select_action(state):
    pass

# Constants: UPPER_SNAKE_CASE
MAX_BUFFER_SIZE = 100000

# Private methods: _leading_underscore
def _compute_loss(self):
    pass
```

### Documentation Standards

```python
def update(self) -> Optional[float]:
    """
    Perform one gradient descent step.
    
    Samples a batch from replay buffer, computes TD-error,
    and updates the policy network. Updates target network
    periodically.
    
    Returns:
        Loss value if update performed, None if buffer too small
        
    Raises:
        RuntimeError: If batch sampling fails
    """
    pass
```

### Error Handling

```python
# Check preconditions
if len(self.replay_buffer) < self.batch_size:
    return None

# Use meaningful exceptions
try:
    batch = self.replay_buffer.sample(self.batch_size)
except ValueError as e:
    raise RuntimeError(f"Failed to sample batch: {e}")

# Clean up resources
try:
    loss.backward()
finally:
    self.optimizer.zero_grad()
```

## ğŸ¯ Performance Baselines

### Expected Results

| Metric | Target | Good | Excellent |
|--------|--------|------|-----------|
| Mean Reward | >200 | >250 | >280 |
| Success Rate | >90% | >95% | >98% |
| Episodes to Solve | <1500 | <1000 | <800 |
| Training Time (GPU) | <2h | <1h | <45min |

### Comparison with Other Algorithms

| Algorithm | Mean Reward | Training Time | Stability |
|-----------|-------------|---------------|-----------|
| DQN | 250 | 1h | â˜…â˜…â˜…â˜†â˜† |
| Double DQN | 260 | 1h | â˜…â˜…â˜…â˜…â˜† |
| Dueling DQN | 265 | 1.2h | â˜…â˜…â˜…â˜…â˜† |
| Rainbow | 280 | 2h | â˜…â˜…â˜…â˜…â˜… |
| PPO | 270 | 45min | â˜…â˜…â˜…â˜…â˜… |

---

For questions or clarifications, please open an issue on GitHub.